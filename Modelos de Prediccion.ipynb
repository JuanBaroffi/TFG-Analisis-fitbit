{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26d4b95",
   "metadata": {},
   "source": [
    "# Modelos de Predicción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a877e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones necesarias para el correcto funcionamiento de todos lo modelos y demás operaciones con los datos\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sklearn\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "222cef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Id                 Time   HeartRate  Intensity  Calories\n",
      "0       2022484408  2016-04-12 07:21:00  101.600000          1   3.32064\n",
      "1       2022484408  2016-04-12 07:22:00   87.888889          1   3.94326\n",
      "2       2022484408  2016-04-12 07:23:00   58.000000          0   1.34901\n",
      "3       2022484408  2016-04-12 07:24:00   58.000000          0   1.03770\n",
      "4       2022484408  2016-04-12 07:25:00   56.777778          0   1.03770\n",
      "...            ...                  ...         ...        ...       ...\n",
      "333141  8877689391  2016-05-12 13:55:00   60.666667          0   1.33353\n",
      "333142  8877689391  2016-05-12 13:56:00   61.875000          0   1.33353\n",
      "333143  8877689391  2016-05-12 13:57:00   58.142857          0   1.33353\n",
      "333144  8877689391  2016-05-12 13:58:00   61.200000          0   1.33353\n",
      "333145  8877689391  2016-05-12 13:59:00   58.000000          0   1.33353\n",
      "\n",
      "[333146 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.read_csv(\"Fitabase Data 4.12.16-5.12.16/test_train_data.csv\")\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50067c70",
   "metadata": {},
   "source": [
    "# Preprocesamiento datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7823c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Convertir la columna 'Time' a tipo datetime en data_intensities_minutes\n",
    "result_df['Time'] = pd.to_datetime(result_df['Time'])\n",
    "\n",
    "# Extraemos características de la columna 'Time'\n",
    "result_df['Hour'] = result_df['Time'].dt.hour\n",
    "result_df['Minutes'] = result_df['Time'].dt.minute\n",
    "result_df['Weekday'] = result_df['Time'].dt.weekday\n",
    "\n",
    "# Definir las características y la variable objetivo\n",
    "features = ['Id', 'Hour', 'Minutes', 'Intensity', 'Calories']\n",
    "target = 'HeartRate'\n",
    "\n",
    "# Separar las características y la variable objetivo\n",
    "X = result_df[features]\n",
    "y = result_df[target]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.17, random_state=777)\n",
    "\n",
    "# Crear un transformador para manejar las variables categóricas y escalar las numéricas\n",
    "categorical_features = ['Id']\n",
    "numeric_features = ['Hour', 'Minutes', 'Intensity', 'Calories']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae1ffd3",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8199d93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pip', 'install', 'xgboost'], returncode=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(['pip', 'install', 'xgboost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81d7a525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94;1mR2 Score: 0.861030\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor(objective='reg:squarederror', random_state=42) \n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular el R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"\\033[94;1mR2 Score: {r2:.6f}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c58919e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mMejores hiperparámetros: {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200, 'subsample': 1.0}\u001b[0m\n",
      "\u001b[94;1mR2 Score: 0.863465\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "best_params = {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200, 'subsample': 1.0}\n",
    "\n",
    "# Crear el modelo XGBoost con los mejores hiperparámetros\n",
    "optimal_model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    subsample=best_params['subsample'],\n",
    "    colsample_bytree=best_params['colsample_bytree']\n",
    ")\n",
    "\n",
    "# Entrenar el modelo óptimo\n",
    "optimal_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba con el mejor modelo\n",
    "y_pred = optimal_model.predict(X_test)\n",
    "\n",
    "# Calcular el R2 score con el mejor modelo\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\033[91m\" + f\"Mejores hiperparámetros: {best_params}\" + \"\\033[0m\")\n",
    "print(f\"\\033[94;1mR2 Score: {r2:.6f}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f18cb",
   "metadata": {},
   "source": [
    "# LSTM: Long Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e576a14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pip', 'install', 'tensorflow', 'scikit-learn'], returncode=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(['pip', 'install', 'tensorflow', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d5c48e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "8641/8641 [==============================] - 12s 1ms/step - loss: 410.5307\n",
      "Epoch 2/15\n",
      "8641/8641 [==============================] - 11s 1ms/step - loss: 119.9345\n",
      "Epoch 3/15\n",
      "8641/8641 [==============================] - 11s 1ms/step - loss: 118.5588\n",
      "Epoch 4/15\n",
      "8641/8641 [==============================] - 11s 1ms/step - loss: 117.6041\n",
      "Epoch 5/15\n",
      "8641/8641 [==============================] - 12s 1ms/step - loss: 116.6333\n",
      "Epoch 6/15\n",
      "8641/8641 [==============================] - 11s 1ms/step - loss: 115.6138\n",
      "Epoch 7/15\n",
      "8641/8641 [==============================] - 12s 1ms/step - loss: 114.8255\n",
      "Epoch 8/15\n",
      "8641/8641 [==============================] - 11s 1ms/step - loss: 114.1608\n",
      "Epoch 9/15\n",
      "8641/8641 [==============================] - 13s 1ms/step - loss: 113.6956\n",
      "Epoch 10/15\n",
      "8641/8641 [==============================] - 12s 1ms/step - loss: 113.3364\n",
      "Epoch 11/15\n",
      "8641/8641 [==============================] - 13s 1ms/step - loss: 113.1273\n",
      "Epoch 12/15\n",
      "8641/8641 [==============================] - 12s 1ms/step - loss: 112.9033\n",
      "Epoch 13/15\n",
      "8641/8641 [==============================] - 12s 1ms/step - loss: 112.7294\n",
      "Epoch 14/15\n",
      "8641/8641 [==============================] - 12s 1ms/step - loss: 112.5748\n",
      "Epoch 15/15\n",
      "8641/8641 [==============================] - 13s 1ms/step - loss: 112.4505\n",
      "1770/1770 [==============================] - 2s 851us/step\n",
      "\u001b[94;1mR2 Score: 0.591548\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test_scaled = scaler.transform(X_test[numeric_features])\n",
    "\n",
    "# Reshape para el formato de entrada de LSTM (n_samples, time steps, features)\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# Construir el modelo LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(1, len(numeric_features))))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train_reshaped, y_train, epochs=15, batch_size=32, verbose=1)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "# Calcular el R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"\\033[94;1mR2 Score: {r2:.6f}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f17156",
   "metadata": {},
   "source": [
    "# MLPRegressor: MultiLayer Perceptron Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ba1b4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94;1m R2 Score: 0.591548\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "# Crear el pipeline con preprocesamiento y el modelo MLPRegressor\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                     ('regressor', MLPRegressor(random_state=42, max_iter=500))])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print(f\"\\033[94;1m R2 Score: {r2:.6f}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dabed2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mMejores hiperparámetros: {'regressor__activation': 'tanh', 'regressor__alpha': 0.001, 'regressor__hidden_layer_sizes': (100, 50)}\u001b[0m\n",
      "\u001b[94;1mR2 score con mejores hiperparámetros: {'regressor__activation': 'tanh', 'regressor__alpha': 0.001, 'regressor__hidden_layer_sizes': (100, 50)} - R2 Score: 0.856033\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\software\\programacion\\python\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "best_params = {'regressor__activation': 'tanh', 'regressor__alpha': 0.001, 'regressor__hidden_layer_sizes': (100, 50)}\n",
    "\n",
    "# Crear el modelo MLPRegressor con los mejores hiperparámetros\n",
    "mlp = MLPRegressor(random_state=42, max_iter=500, \n",
    "                   activation=best_params['regressor__activation'], \n",
    "                   alpha=best_params['regressor__alpha'], \n",
    "                   hidden_layer_sizes=best_params['regressor__hidden_layer_sizes'])\n",
    "\n",
    "# Crear el pipeline con preprocesamiento y el modelo MLPRegressor\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                             ('regressor', mlp)])\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo con los mejores hiperparámetros\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(\"\\033[91m\" + f\"Mejores hiperparámetros: {best_params}\" + \"\\033[0m\")\n",
    "print(\"\\033[94;1m\" + f\"R2 score con mejores hiperparámetros: {best_params} - R2 Score: {score:.6f}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0839ae",
   "metadata": {},
   "source": [
    "# KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0744e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "numeric_features = ['Hour', 'Minutes', 'Intensity', 'Calories']\n",
    "categorical_features = ['Id']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ], remainder='passthrough')  # 'passthrough' mantiene los datos dispersos\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=5, algorithm='brute')  # Puedes probar con 'ball_tree' o 'kd_tree' si persiste el problema\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4d8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors_range = np.arange(1, 21)  # Explorar valores entre 1 y 20\n",
    "\n",
    "# Crear un diccionario con los hiperparámetros a ajustar\n",
    "param_grid = {'model__n_neighbors': n_neighbors_range}  # Usa la notación 'model__' para acceder al parámetro dentro del Pipeline\n",
    "\n",
    "# Crear el modelo KNN Regressor\n",
    "model = KNeighborsRegressor()\n",
    "\n",
    "# Crear el pipeline que combina el preprocesamiento y el modelo\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Asume que tienes el preprocessor ya definido\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Usar GridSearchCV para encontrar la mejor combinación de hiperparámetros\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2')  # Validación cruzada con 5 folds\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Crear el modelo KNN Regressor con los mejores hiperparámetros\n",
    "model = KNeighborsRegressor(**best_params)  # Desempaqueta los parámetros\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "score = model.score(X_test, y_test)\n",
    "print(f\"\\033[94;1mR2 Score (con mejores hiperparámetros): {score:.4f}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f424c3",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bff77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline  # Make sure to import Pipeline\n",
    "\n",
    "# DecisionTreeRegressor Model\n",
    "dt_regressor = DecisionTreeRegressor(random_state=11101)\n",
    "\n",
    "# Construir el pipeline con el transformador y el modelo\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', dt_regressor)\n",
    "])  # Add a closing parenthesis here\n",
    "\n",
    "# Cross Validation execution\n",
    "scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "mse_scores = -scores  # Convertir a positivo, ya que cross_val_score devuelve la negación del MSE\n",
    "\n",
    "# Entrenar el modelo en el conjunto de entrenamiento\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on Test Set: {mse_test}')\n",
    "\n",
    "# Matriz de confusión\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(np.round(np.array([[mse_test]]), decimals=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e00256",
   "metadata": {},
   "source": [
    "# SVM: Support Vector Machine Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e586639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Definir las características y la variable objetivo\n",
    "features = ['Hour', 'Minutes', 'Intensity', 'Calories']\n",
    "target = 'HeartRate'\n",
    "\n",
    "# Separar las características y la variable objetivo\n",
    "X = result_df[features]\n",
    "y = result_df[target]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=11101)\n",
    "\n",
    "# Crear el modelo SVM Regressor\n",
    "model = SVR(kernel='rbf', gamma='auto', epsilon=0.1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(\"R2 score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc773a",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06d999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Crear el modelo de Random Forest\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=11101)\n",
    "\n",
    "# Crear el pipeline que combina el preprocesamiento y el modelo\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "score = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'R2 Score on Test Set: {score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
