{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebd8e60",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ADD8E6; border-radius: 12px; padding: 20px; border: 3px solid #87CEEB;\">\n",
    "    <strong style=\"color: #000000; font-size: 1.5em;\">Modelos de predicción</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a877e38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\34634\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\34634\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# Importaciones necesarias para el correcto funcionamiento de todos lo modelos y demás operaciones con los datos\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sklearn\n",
    "import warnings\n",
    "import subprocess\n",
    "import importlib\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "222cef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Id                 Time   HeartRate  Intensity  Calories\n",
      "0       2022484408  2016-04-12 07:21:00  101.600000          1   3.32064\n",
      "1       2022484408  2016-04-12 07:22:00   87.888889          1   3.94326\n",
      "2       2022484408  2016-04-12 07:23:00   58.000000          0   1.34901\n",
      "3       2022484408  2016-04-12 07:24:00   58.000000          0   1.03770\n",
      "4       2022484408  2016-04-12 07:25:00   56.777778          0   1.03770\n",
      "...            ...                  ...         ...        ...       ...\n",
      "333141  8877689391  2016-05-12 13:55:00   60.666667          0   1.33353\n",
      "333142  8877689391  2016-05-12 13:56:00   61.875000          0   1.33353\n",
      "333143  8877689391  2016-05-12 13:57:00   58.142857          0   1.33353\n",
      "333144  8877689391  2016-05-12 13:58:00   61.200000          0   1.33353\n",
      "333145  8877689391  2016-05-12 13:59:00   58.000000          0   1.33353\n",
      "\n",
      "[333146 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.read_csv(\"Fitabase Data 4.12.16-5.12.16/test_train_data.csv\")\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae1ffd3",
   "metadata": {},
   "source": [
    "# Instalación/Actualización de librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ebc6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8199d93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost ya está instalado en el sistema.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    importlib.import_module('xgboost')\n",
    "    print(\"XGBoost ya está instalado en el sistema.\")\n",
    "except ImportError:\n",
    "    subprocess.run(['pip', 'install', 'xgboost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7106e1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM ya está instalado en el sistema.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    importlib.import_module('xgboost')\n",
    "    print(\"LightGBM ya está instalado en el sistema.\")\n",
    "except ImportError:\n",
    "    subprocess.run(['pip', 'install', 'lightgbm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e97fb1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow ya está instalado en el sistema.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    importlib.import_module('xgboost')\n",
    "    print(\"TensorFlow ya está instalado en el sistema.\")\n",
    "except ImportError:\n",
    "    subprocess.run(['pip', 'install', 'tensorflow', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e619aa1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pip', 'install', '--upgrade', 'lightgbm', 'pandas', 'dask'], returncode=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(['pip', 'install', '--upgrade', 'lightgbm', 'pandas', 'dask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebdc0631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyArrow versión 15.0.0 ya está instalada.\n"
     ]
    }
   ],
   "source": [
    "required_version = '14.0.1'\n",
    "\n",
    "try:\n",
    "    pyarrow_version = importlib.import_module('pyarrow').__version__\n",
    "    if version.parse(pyarrow_version) < version.parse(required_version):\n",
    "        print(f\"PyArrow versión {pyarrow_version} encontrada. Actualizando a la versión {required_version}.\")\n",
    "        subprocess.run(['pip', 'install', f'pyarrow>={required_version}'])\n",
    "    else:\n",
    "        print(f\"PyArrow versión {pyarrow_version} ya está instalada.\")\n",
    "except ImportError:\n",
    "    subprocess.run(['pip', 'install', f'pyarrow>={required_version}'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b0a6a6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ADD8E6; border-radius: 12px; padding: 20px; border: 3px solid #87CEEB;\">\n",
    "    <strong style=\"color: #000000; font-size: 1.5em;\">⚠ **User Alert:**</strong> Don't forget to RESTART the kernel for the magic to happen after ADOPTING the new libraries.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07b1456",
   "metadata": {},
   "source": [
    "# Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5aa142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Convertir la columna 'Time' a tipo datetime en data_intensities_minutes\n",
    "result_df['Time'] = pd.to_datetime(result_df['Time'])\n",
    "\n",
    "# Extraemos características de la columna 'Time'\n",
    "result_df['Hour'] = result_df['Time'].dt.hour\n",
    "result_df['Minutes'] = result_df['Time'].dt.minute\n",
    "result_df['Weekday'] = result_df['Time'].dt.weekday\n",
    "\n",
    "# Definir las características y la variable objetivo\n",
    "features = ['Id', 'Hour', 'Minutes', 'Intensity', 'Calories']\n",
    "target = 'HeartRate'\n",
    "\n",
    "# Separar las características y la variable objetivo\n",
    "X = result_df[features]\n",
    "y = result_df[target]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.17, random_state=777)\n",
    "\n",
    "# Crear un transformador para manejar las variables categóricas y escalar las numéricas\n",
    "categorical_features = ['Id']\n",
    "numeric_features = ['Hour', 'Minutes', 'Intensity', 'Calories']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478f5f0c",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81d7a525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94;1mPromedio R2 Score (XGBoost): 0.858344518126301\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "num_splits = 5 \n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgb_scores = cross_val_score(xgb_model, X, y, scoring='r2', cv=KFold(n_splits=num_splits, shuffle=True, random_state=42))\n",
    "\n",
    "print(f\"\\033[94;1mPromedio R2 Score (XGBoost): {xgb_scores.mean()}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c58919e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94;1mPromedio R2 Score (XGBoost) con mejores hiperparámetros: 0.8604436463886229\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Mejores hiperparámetros encontrados previamente utilizado GridSearch\n",
    "best_params = {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200, 'subsample': 1.0}\n",
    "\n",
    "# Crear el modelo XGBoost con los mejores hiperparámetros\n",
    "optimal_model = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    subsample=best_params['subsample'],\n",
    "    colsample_bytree=best_params['colsample_bytree']\n",
    ")\n",
    "\n",
    "num_splits = 5\n",
    "xgb_scores = cross_val_score(optimal_model, X, y, scoring='r2', cv=KFold(n_splits=num_splits, shuffle=True, random_state=42))\n",
    "print(f\"\\033[94;1mPromedio R2 Score (XGBoost) con mejores hiperparámetros: {xgb_scores.mean()}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70565d5e",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc036201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\34634\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:150: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] El sistema no puede encontrar el archivo especificado\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\34634\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 227, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"C:\\Users\\34634\\anaconda3\\lib\\subprocess.py\", line 505, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"C:\\Users\\34634\\anaconda3\\lib\\subprocess.py\", line 951, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\34634\\anaconda3\\lib\\subprocess.py\", line 1420, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 358\n",
      "[LightGBM] [Info] Number of data points in the train set: 266516, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 73.702794\n",
      "[LightGBM] [Info] Total Bins 358\n",
      "[LightGBM] [Info] Number of data points in the train set: 266517, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 73.701315\n",
      "[LightGBM] [Info] Total Bins 358\n",
      "[LightGBM] [Info] Number of data points in the train set: 266517, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 73.703051\n",
      "[LightGBM] [Info] Total Bins 358\n",
      "[LightGBM] [Info] Number of data points in the train set: 266517, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 73.734438\n",
      "[LightGBM] [Info] Total Bins 358\n",
      "[LightGBM] [Info] Number of data points in the train set: 266517, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 73.707150\n",
      "\u001b[94;1mPromedio R2 Score (LightGBM): 0.8608125059372875\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Mejores hiperparámetros encontrados previamente con GridSearch\n",
    "best_params = {'learning_rate': 0.2, 'n_estimators': 200, 'num_leaves': 40}\n",
    "\n",
    "best_model = lgb.LGBMRegressor(objective='regression', random_state=42, force_row_wise=True, **best_params)\n",
    "\n",
    "num_splits = 5 \n",
    "lgb_scores = cross_val_score(best_model, X, y, scoring='r2', cv=KFold(n_splits=num_splits, shuffle=True, random_state=42))\n",
    "\n",
    "print(f\"\\033[94;1mPromedio R2 Score (LightGBM): {lgb_scores.mean()}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cc66721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 374\n",
      "[LightGBM] [Info] Number of data points in the train set: 266516, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 73.702794\n",
      "[LightGBM] [Info] Total Bins 374\n",
      "[LightGBM] [Info] Number of data points in the train set: 266517, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 73.701315\n",
      "[LightGBM] [Info] Total Bins 374\n",
      "[LightGBM] [Info] Number of data points in the train set: 266517, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 73.703051\n",
      "[LightGBM] [Info] Total Bins 374\n",
      "[LightGBM] [Info] Number of data points in the train set: 266517, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 73.734438\n",
      "[LightGBM] [Info] Total Bins 374\n",
      "[LightGBM] [Info] Number of data points in the train set: 266517, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score 73.707150\n",
      "\u001b[94;1mPromedio R2 Score (LightGBM): 0.8555762087754732\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Mejores hiperparámetros encontrados previamente con GridSearch\n",
    "best_params = {'learning_rate': 0.2, 'n_estimators': 200, 'num_leaves': 40}\n",
    "\n",
    "best_model = lgb.LGBMRegressor(objective='regression', random_state=42, **best_params)\n",
    "\n",
    "num_splits = 5 \n",
    "lgb_params = best_params.copy()\n",
    "lgb_params['force_row_wise'] = True\n",
    "\n",
    "best_model_force_row_wise = lgb.LGBMRegressor(objective='regression', random_state=42, **lgb_params)\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                             ('regressor', best_model_force_row_wise)])\n",
    "lgb_scores = cross_val_score(pipeline, X, y, scoring='r2', cv=KFold(n_splits=num_splits, shuffle=True, random_state=42))\n",
    "\n",
    "print(f\"\\033[94;1mPromedio R2 Score (LightGBM): {lgb_scores.mean()}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f18cb",
   "metadata": {},
   "source": [
    "# LSTM: Long Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d5c48e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\34634\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\34634\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\34634\\anaconda3\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:From C:\\Users\\34634\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "8641/8641 [==============================] - 31s 3ms/step - loss: 375.6779\n",
      "Epoch 2/15\n",
      "8641/8641 [==============================] - 33s 4ms/step - loss: 120.2599\n",
      "Epoch 3/15\n",
      "8641/8641 [==============================] - 30s 3ms/step - loss: 118.7805\n",
      "Epoch 4/15\n",
      "8641/8641 [==============================] - 28s 3ms/step - loss: 117.7508\n",
      "Epoch 5/15\n",
      "8641/8641 [==============================] - 28s 3ms/step - loss: 116.7752\n",
      "Epoch 6/15\n",
      "8641/8641 [==============================] - 32s 4ms/step - loss: 115.7455\n",
      "Epoch 7/15\n",
      "8641/8641 [==============================] - 27s 3ms/step - loss: 114.8084\n",
      "Epoch 8/15\n",
      "8641/8641 [==============================] - 29s 3ms/step - loss: 114.1138\n",
      "Epoch 9/15\n",
      "8641/8641 [==============================] - 28s 3ms/step - loss: 113.5860\n",
      "Epoch 10/15\n",
      "8641/8641 [==============================] - 26s 3ms/step - loss: 113.2573\n",
      "Epoch 11/15\n",
      "8641/8641 [==============================] - 23s 3ms/step - loss: 112.9817\n",
      "Epoch 12/15\n",
      "8641/8641 [==============================] - 23s 3ms/step - loss: 112.7974\n",
      "Epoch 13/15\n",
      "8641/8641 [==============================] - 24s 3ms/step - loss: 112.5899\n",
      "Epoch 14/15\n",
      "8641/8641 [==============================] - 23s 3ms/step - loss: 112.3994\n",
      "Epoch 15/15\n",
      "8641/8641 [==============================] - 25s 3ms/step - loss: 112.3219\n",
      "1770/1770 [==============================] - 4s 2ms/step\n",
      "\u001b[94;1mR2 Score: 0.592656\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[numeric_features])\n",
    "X_test_scaled = scaler.transform(X_test[numeric_features])\n",
    "\n",
    "# Reshape para el formato de entrada de LSTM (n_samples, time steps, features)\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# Construir el modelo LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(120, activation='relu', input_shape=(1, len(numeric_features))))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train_reshaped, y_train, epochs=15, batch_size=32, verbose=1)\n",
    "\n",
    "y_pred = model.predict(X_test_reshaped)\n",
    "\n",
    "# Calcular el R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"\\033[94;1mR2 Score: {r2:.6f}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f17156",
   "metadata": {},
   "source": [
    "# MLPRegressor: MultiLayer Perceptron Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba1b4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\34634\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "C:\\Users\\34634\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                             ('regressor', MLPRegressor(random_state=42, max_iter=1000))])\n",
    "num_splits = 5  \n",
    "\n",
    "# Validación cruzada \n",
    "mlp_scores = cross_val_score(pipeline, X, y, scoring='r2', cv=KFold(n_splits=num_splits, shuffle=True, random_state=42))\n",
    "\n",
    "print(f\"R2 Scores (MLPRegressor) en {num_splits}-fold cross-validation: {mlp_scores}\")\n",
    "print(f\"Promedio R2 Score (MLPRegressor): {mlp_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dabed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', MLPRegressor(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'regressor__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "    'regressor__alpha': [0.0001, 0.001, 0.01],\n",
    "    'regressor__activation': ['relu', 'tanh'],\n",
    "}\n",
    "\n",
    "# Define the custom scoring function (R2 score)\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "\n",
    "# Perform grid search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=KFold(n_splits=5, shuffle=True, random_state=42), scoring=r2_scorer)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new MLPRegressor with the best hyperparameters\n",
    "best_model = MLPRegressor(random_state=42, max_iter=1000, **best_params)\n",
    "\n",
    "# Perform cross-validation with the best model\n",
    "num_splits = 5\n",
    "best_mlp_scores = cross_val_score(best_model, X, y, scoring='r2', cv=KFold(n_splits=num_splits, shuffle=True, random_state=42))\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"R2 Scores (MLPRegressor) with Best Hyperparameters in {num_splits}-fold cross-validation: {best_mlp_scores}\")\n",
    "print(f\"Average R2 Score (MLPRegressor) with Best Hyperparameters: {best_mlp_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0839ae",
   "metadata": {},
   "source": [
    "# KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0744e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "numeric_features = ['Hour', 'Minutes', 'Intensity', 'Calories']\n",
    "categorical_features = ['Id']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ], remainder='passthrough') \n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=5, algorithm='brute')  # Puedes probar con 'ball_tree' o 'kd_tree' si persiste el problema\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "num_splits = 5\n",
    "knn_scores = cross_val_score(pipeline, X, y, scoring='r2', cv=KFold(n_splits=num_splits, shuffle=True, random_state=42))\n",
    "\n",
    "print(f\"Promedio R2 Score (KNeighborsRegressor): {knn_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4d8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors_range = np.arange(1, 21)\n",
    "\n",
    "param_grid = {'model__n_neighbors': n_neighbors_range} \n",
    "\n",
    "model = KNeighborsRegressor()\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2')  # Validación cruzada con 5 folds\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Crear el modelo KNN Regressor con los mejores hiperparámetros\n",
    "model = KNeighborsRegressor(**best_params)  # Desempaqueta los parámetros\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "score = model.score(X_test, y_test)\n",
    "print(f\"\\033[94;1mR2 Score (con mejores hiperparámetros): {score:.6f}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f424c3",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bff77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt_regressor = DecisionTreeRegressor(random_state=11101)\n",
    "\n",
    "# Construct the pipeline with the preprocessor and the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', dt_regressor)\n",
    "])\n",
    "\n",
    "# Cross-validation execution\n",
    "num_splits = 5\n",
    "scores = cross_val_score(pipeline, X_train, y_train, cv=num_splits, scoring='neg_mean_squared_error')\n",
    "mse_scores = -scores  # Convert to positive, as cross_val_score returns the negation of MSE\n",
    "\n",
    "# Train the model on the training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error on the test set\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on Test Set: {mse_test}')\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(np.round(np.array([[mse_test]]), decimals=2))\n",
    "\n",
    "# Additional information about cross-validation scores\n",
    "print(f'Mean MSE across {num_splits}-fold Cross-Validation: {mse_scores.mean()}')\n",
    "print(f'MSE Scores for each fold: {mse_scores}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e00256",
   "metadata": {},
   "source": [
    "# SVM: Support Vector Machine Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e586639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "model = SVR(kernel='rbf', gamma='auto', epsilon=0.1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(\"R2 score on the test set:\", score)\n",
    "\n",
    "# Cross-validation\n",
    "num_splits = 5\n",
    "svm_scores = cross_val_score(model, X, y, scoring='r2', cv=KFold(n_splits=num_splits, shuffle=True, random_state=11101))\n",
    "\n",
    "print(f\"R2 Scores (SVR) in {num_splits}-fold cross-validation: {svm_scores}\")\n",
    "print(f\"Average R2 Score (SVR) across {num_splits}-fold cross-validation: {svm_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc773a",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06d999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=11101)\n",
    "\n",
    "# Create the pipeline that combines preprocessing and the model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "score = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'R2 Score on Test Set: {score}')\n",
    "\n",
    "# Cross-validation\n",
    "num_splits = 5\n",
    "rf_scores = cross_val_score(pipeline, X, y, scoring='r2', cv=KFold(n_splits=num_splits, shuffle=True, random_state=11101))\n",
    "\n",
    "print(f\"R2 Scores (Random Forest) in {num_splits}-fold cross-validation: {rf_scores}\")\n",
    "print(f\"Average R2 Score (Random Forest) across {num_splits}-fold cross-validation: {rf_scores.mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
