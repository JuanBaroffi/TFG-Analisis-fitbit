{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4401fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Librería para analizar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adfe5729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Id                  Time  Value\n",
      "0  2022484408  4/12/2016 7:21:00 AM     97\n",
      "1  2022484408  4/12/2016 7:21:05 AM    102\n",
      "2  2022484408  4/12/2016 7:21:10 AM    105\n",
      "3  2022484408  4/12/2016 7:21:20 AM    103\n",
      "4  2022484408  4/12/2016 7:21:25 AM    101\n"
     ]
    }
   ],
   "source": [
    "# Carga el archivo CSV\n",
    "data_heartrate_seconds = pd.read_csv(\"Fitabase Data 4.12.16-5.12.16/heartrate_seconds_merged.csv\")\n",
    "print(data_heartrate_seconds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "519b1120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07ad865d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id       0\n",
       "Time     0\n",
       "Value    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra el número de valores NaN en cada columna\n",
    "data_heartrate_seconds.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aa4ec21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id       0\n",
       "Time     0\n",
       "Value    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "mode_imputer = SimpleImputer(strategy = 'most_frequent') # moda\n",
    "\n",
    "# Hacer inputación por cada columna, sustituyendo aquellos valores Nan con la moda previamente calculada\n",
    "for column in data_heartrate_seconds.columns:\n",
    "    values = data_heartrate_seconds[column].values.reshape(-1,1)\n",
    "    mode_imputer.fit(values)\n",
    "    data_heartrate_seconds[column] = mode_imputer.transform(values)\n",
    "\n",
    "# Comprobamos que hayamos eliminadolos atípicos o erroneos.\n",
    "data_heartrate_seconds.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96322b2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convierte la columna de fecha a formato datetime\n",
    "data_heartrate_seconds['Time'] = pd.to_datetime(data_heartrate_seconds['Time'])\n",
    "\n",
    "# Agrupa por ID y fecha redondeada al minuto, calcula la media y reinicia el índice\n",
    "result_df = data_heartrate_seconds.groupby(['Id', pd.Grouper(key='Time', freq='1Min')]).mean().reset_index()\n",
    "\n",
    "# Guarda el resultado en un nuevo archivo CSV\n",
    "result_df.to_csv(\"Fitabase Data 4.12.16-5.12.16/heartrate_minutes_merged.csv\", index=False)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f578c7aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Id         ActivityMinute  Intensity\n",
      "0  1503960366  4/12/2016 12:00:00 AM          0\n",
      "1  1503960366  4/12/2016 12:01:00 AM          0\n",
      "2  1503960366  4/12/2016 12:02:00 AM          0\n",
      "3  1503960366  4/12/2016 12:03:00 AM          0\n",
      "4  1503960366  4/12/2016 12:04:00 AM          0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Id                0\n",
       "ActivityMinute    0\n",
       "Intensity         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga el archivo CSV\n",
    "data_intensities_minutes = pd.read_csv(\"Fitabase Data 4.12.16-5.12.16/minuteIntensitiesNarrow_merged.csv\")\n",
    "print(data_intensities_minutes.head())\n",
    "# Muestra el número de valores NaN en cada columna\n",
    "data_intensities_minutes.isna().sum()\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "mode_imputer = SimpleImputer(strategy = 'most_frequent') # moda\n",
    "\n",
    "# Hacer inputación por cada columna\n",
    "for column in data_intensities_minutes.columns:\n",
    "    values = data_intensities_minutes[column].values.reshape(-1,1)\n",
    "    mode_imputer.fit(values)\n",
    "    data_intensities_minutes[column] = mode_imputer.transform(values)\n",
    "\n",
    "# Comprobamos que hayamos eliminadolos atípicos o erroneos.\n",
    "data_intensities_minutes.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebf66c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el archivo CSV\n",
    "#data_dailyActivity_merged = pd.read_csv(\"Fitabase Data 4.12.16-5.12.16/dailyActivity_merged.csv\")\n",
    "#print(data_intensities_minutes.head())\n",
    "# Muestra el número de valores NaN en cada columna\n",
    "#data_intensities_minutes.isna().sum()\n",
    "\n",
    "#from sklearn.impute import SimpleImputer\n",
    "#mode_imputer = SimpleImputer(strategy = 'most_frequent') # moda\n",
    "\n",
    "# Hacer inputación por cada columna\n",
    "#for column in data_intensities_minutes.columns:\n",
    " #   values = data_intensities_minutes[column].values.reshape(-1,1)\n",
    "  #  mode_imputer.fit(values)\n",
    "  #  data_intensities_minutes[column] = mode_imputer.transform(values)\n",
    "\n",
    "# Comprobamos que hayamos eliminadolos atípicos o erroneos.\n",
    "#data_intensities_minutes.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbcff66",
   "metadata": {},
   "source": [
    "Sleep Enum 1->Asleep 2-->Restless 3-->Awake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a44c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el archivo CSV\n",
    "#data_minutes_sleep = pd.read_csv(\"Fitabase Data 4.12.16-5.12.16/minuteSleep_merged.csv\")#\n",
    "#print(data_minutes_sleep.head())\n",
    "# Muestra el número de valores NaN en cada columna\n",
    "#data_minutes_sleep.isna().sum()\n",
    "\n",
    "#from sklearn.impute import SimpleImputer\n",
    "#mode_imputer = SimpleImputer(strategy = 'most_frequent') # moda\n",
    "\n",
    "# Hacer inputación por cada columna\n",
    "#for column in data_minutes_sleep.columns:\n",
    " #   values = data_minutes_sleep[column].values.reshape(-1,1)\n",
    "  #  mode_imputer.fit(values)\n",
    "   # data_minutes_sleep[column] = mode_imputer.transform(values)\n",
    "\n",
    "# Comprobamos que hayamos eliminadolos atípicos o erroneos.\n",
    "#data_minutes_sleep.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a1fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convertir la columna 'date' a tipo datetime\n",
    "#data_minutes_sleep['date'] = pd.to_datetime(data_minutes_sleep['date'])\n",
    "\n",
    "# Restar 30 segundos a cada entrada\n",
    "#data_minutes_sleep['date'] = data_minutes_sleep['date'] - pd.Timedelta(seconds=30)\n",
    "\n",
    "# Ajustar minutos y segundos a 00\n",
    "#data_minutes_sleep['date'] = data_minutes_sleep['date'].dt.floor('T')\n",
    "\n",
    "# Guardar el nuevo dataset en un nuevo archivo CSV\n",
    "#data_minutes_sleep.to_csv(\"minuteSleep_merged.csv\", index=False)\n",
    "\n",
    "# Elimina la columna 'LogId'\n",
    "#data_minutes_sleep = data_minutes_sleep.drop(columns=['logId'])\n",
    "\n",
    "# Guarda el DataFrame modificado de nuevo en el archivo CSV original\n",
    "#data_minutes_sleep.to_csv(\"minuteSleep_merged.csv\", index=False)\n",
    "\n",
    "#print(\"La columna 'LogId' se ha eliminado exitosamente.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a34c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_minutes_sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                0\n",
      "ActivityMinute    0\n",
      "Calories          0\n",
      "dtype: int64\n",
      "Id                0\n",
      "ActivityMinute    0\n",
      "Calories          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo CSV de calorías\n",
    "data_calories = pd.read_csv(\"Fitabase Data 4.12.16-5.12.16/minuteCaloriesNarrow_merged.csv\")\n",
    "\n",
    "#Hacer merged con el 'result_df' para incorporar la nueva columna calories\n",
    "\n",
    "\n",
    "# Muestra el número de valores NaN en cada columna\n",
    "print(data_calories.isna().sum())\n",
    "\n",
    "# Realizar la imputación para los valores faltantes en 'calories'\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')\n",
    "for column in data_calories.columns:\n",
    "    values = data_calories[column].values.reshape(-1, 1)\n",
    "    mode_imputer.fit(values)\n",
    "    data_calories[column] = mode_imputer.transform(values)\n",
    "\n",
    "# Comprobamos que hayamos eliminado los valores atípicos o erróneos.\n",
    "print(data_calories.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcfa2b9a",
   "metadata": {},
   "source": [
    "A continuacion imprimeros todos los dataset, leidos hasta el momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcc4acf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahora imprimeros la pulsaciones por minito (heartrate_seconds_merged.csv.csv) \n",
      "\n",
      "           Id                Time       Value\n",
      "0  2022484408 2016-04-12 07:21:00  101.600000\n",
      "1  2022484408 2016-04-12 07:22:00   87.888889\n",
      "2  2022484408 2016-04-12 07:23:00   58.000000\n",
      "3  2022484408 2016-04-12 07:24:00   58.000000\n",
      "4  2022484408 2016-04-12 07:25:00   56.777778\n",
      "\n",
      "\n",
      "Ahora imprimeros las intensidades por minuto (data_intensities_minutes.csv) \n",
      "\n",
      "           Id         ActivityMinute  Intensity\n",
      "0  1503960366  4/12/2016 12:00:00 AM          0\n",
      "1  1503960366  4/12/2016 12:01:00 AM          0\n",
      "2  1503960366  4/12/2016 12:02:00 AM          0\n",
      "3  1503960366  4/12/2016 12:03:00 AM          0\n",
      "4  1503960366  4/12/2016 12:04:00 AM          0\n",
      "\n",
      "\n",
      "Ahora imprimeros la cabecera de (minuteCaloriesNarrow_merged.csv)\n",
      "           Id         ActivityMinute  Calories\n",
      "0  1503960366  4/12/2016 12:00:00 AM    0.7865\n",
      "1  1503960366  4/12/2016 12:01:00 AM    0.7865\n",
      "2  1503960366  4/12/2016 12:02:00 AM    0.7865\n",
      "3  1503960366  4/12/2016 12:03:00 AM    0.7865\n",
      "4  1503960366  4/12/2016 12:04:00 AM    0.7865\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cambiar el nombre de la variable data del csv heartrate por uno mas explicativo\n",
    "print(\"Ahora imprimeros la pulsaciones por minito (heartrate_seconds_merged.csv.csv) \\n\")\n",
    "print(result_df.head())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Ahora imprimeros las intensidades por minuto (data_intensities_minutes.csv) \\n\")\n",
    "print(data_intensities_minutes.head())\n",
    "\n",
    "print(\"\\n\")\n",
    "#print(\"Ahora imprimeros la cabecera de (minuteSleep_merged.csv)\")\n",
    "#print(data_minutes_sleep.head())\n",
    "\n",
    "print(\"Ahora imprimeros la cabecera de (minuteCaloriesNarrow_merged.csv)\")\n",
    "print(data_calories.head())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna 'ActivityMinute' a tipo datetime\n",
    "data_calories['ActivityMinute'] = pd.to_datetime(data_calories['ActivityMinute'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Id                Time       Value      ActivityMinute  \\\n",
      "0       2022484408 2016-04-12 07:21:00  101.600000 2016-04-12 07:21:00   \n",
      "1       2022484408 2016-04-12 07:22:00   87.888889 2016-04-12 07:22:00   \n",
      "2       2022484408 2016-04-12 07:23:00   58.000000 2016-04-12 07:23:00   \n",
      "3       2022484408 2016-04-12 07:24:00   58.000000 2016-04-12 07:24:00   \n",
      "4       2022484408 2016-04-12 07:25:00   56.777778 2016-04-12 07:25:00   \n",
      "...            ...                 ...         ...                 ...   \n",
      "333141  8877689391 2016-05-12 13:55:00   60.666667 2016-05-12 13:55:00   \n",
      "333142  8877689391 2016-05-12 13:56:00   61.875000 2016-05-12 13:56:00   \n",
      "333143  8877689391 2016-05-12 13:57:00   58.142857 2016-05-12 13:57:00   \n",
      "333144  8877689391 2016-05-12 13:58:00   61.200000 2016-05-12 13:58:00   \n",
      "333145  8877689391 2016-05-12 13:59:00   58.000000 2016-05-12 13:59:00   \n",
      "\n",
      "        Calories  \n",
      "0        3.32064  \n",
      "1        3.94326  \n",
      "2        1.34901  \n",
      "3        1.03770  \n",
      "4        1.03770  \n",
      "...          ...  \n",
      "333141   1.33353  \n",
      "333142   1.33353  \n",
      "333143   1.33353  \n",
      "333144   1.33353  \n",
      "333145   1.33353  \n",
      "\n",
      "[333146 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Fusionar los conjuntos de datos utilizando 'Id' y 'ActivityMinute'\n",
    "#result_df = El dataset de Heartrate\n",
    "merged_df = pd.merge(result_df, data_calories, left_on=['Id', 'Time'], right_on=['Id', 'ActivityMinute'])\n",
    "\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5feb7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Id                Time   HeartRate  Calories\n",
      "0  2022484408 2016-04-12 07:21:00  101.600000   3.32064\n",
      "1  2022484408 2016-04-12 07:22:00   87.888889   3.94326\n",
      "2  2022484408 2016-04-12 07:23:00   58.000000   1.34901\n",
      "3  2022484408 2016-04-12 07:24:00   58.000000   1.03770\n",
      "4  2022484408 2016-04-12 07:25:00   56.777778   1.03770\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar las columnas deseadas y renombrarlas\n",
    "result_df = merged_df[['Id', 'Time', 'Value', 'Calories']]\n",
    "result_df.columns = ['Id', 'Time', 'HeartRate', 'Calories']\n",
    "\n",
    "# Imprimir las primeras filas del conjunto de datos resultante\n",
    "print(result_df.head())\n",
    "\n",
    "# Guardar el nuevo dataset en un nuevo archivo CSV\n",
    "result_df.to_csv(\"Fitabase Data 4.12.16-5.12.16/test_train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26b60ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Id                Time   HeartRate  Calories  \\\n",
      "0       2022484408 2016-04-12 07:21:00  101.600000   3.32064   \n",
      "1       2022484408 2016-04-12 07:22:00   87.888889   3.94326   \n",
      "2       2022484408 2016-04-12 07:23:00   58.000000   1.34901   \n",
      "3       2022484408 2016-04-12 07:24:00   58.000000   1.03770   \n",
      "4       2022484408 2016-04-12 07:25:00   56.777778   1.03770   \n",
      "...            ...                 ...         ...       ...   \n",
      "333141  8877689391 2016-05-12 13:55:00   60.666667   1.33353   \n",
      "333142  8877689391 2016-05-12 13:56:00   61.875000   1.33353   \n",
      "333143  8877689391 2016-05-12 13:57:00   58.142857   1.33353   \n",
      "333144  8877689391 2016-05-12 13:58:00   61.200000   1.33353   \n",
      "333145  8877689391 2016-05-12 13:59:00   58.000000   1.33353   \n",
      "\n",
      "            ActivityMinute  Intensity  \n",
      "0      2016-04-12 07:21:00          1  \n",
      "1      2016-04-12 07:22:00          1  \n",
      "2      2016-04-12 07:23:00          0  \n",
      "3      2016-04-12 07:24:00          0  \n",
      "4      2016-04-12 07:25:00          0  \n",
      "...                    ...        ...  \n",
      "333141 2016-05-12 13:55:00          0  \n",
      "333142 2016-05-12 13:56:00          0  \n",
      "333143 2016-05-12 13:57:00          0  \n",
      "333144 2016-05-12 13:58:00          0  \n",
      "333145 2016-05-12 13:59:00          0  \n",
      "\n",
      "[333146 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convertir la columna 'Time' a tipo datetime en data_intensities_minutes\n",
    "data_intensities_minutes['ActivityMinute'] = pd.to_datetime(data_intensities_minutes['ActivityMinute'])\n",
    "\n",
    "# Fusionar los DataFrames utilizando 'Id' y 'Time'\n",
    "merged_df = pd.merge(result_df, data_intensities_minutes, left_on=['Id', 'Time'], right_on=['Id', 'ActivityMinute'])\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           Id                Time   HeartRate  Intensity  Calories\n",
      "0  2022484408 2016-04-12 07:21:00  101.600000          1   3.32064\n",
      "1  2022484408 2016-04-12 07:22:00   87.888889          1   3.94326\n",
      "2  2022484408 2016-04-12 07:23:00   58.000000          0   1.34901\n",
      "3  2022484408 2016-04-12 07:24:00   58.000000          0   1.03770\n",
      "4  2022484408 2016-04-12 07:25:00   56.777778          0   1.03770\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Seleccionar las columnas deseadas y renombrarlas\n",
    "result_df = merged_df[['Id', 'Time', 'HeartRate', 'Intensity', 'Calories']]\n",
    "\n",
    "#Printeamos el nuevo dataset\n",
    "print()\n",
    "print(result_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4405dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el nuevo dataset en un nuevo archivo CSV\n",
    "result_df.to_csv(\"Fitabase Data 4.12.16-5.12.16/test_train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_minutes_sleep.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de56cdc",
   "metadata": {},
   "source": [
    "A continuación, procederemos con el entrenamiento del los distintos modelos utilizados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "142bbbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Extraemos características de la columna 'Time'\n",
    "result_df['Hour'] = result_df['Time'].dt.hour\n",
    "result_df['Minutes'] = result_df['Time'].dt.minute\n",
    "result_df['Weekday'] = result_df['Time'].dt.weekday\n",
    "\n",
    "# Definir las características y la variable objetivo\n",
    "features = ['Id',  'Hour', 'Minutes', 'Intensity', 'Calories']\n",
    "target = 'HeartRate'\n",
    "\n",
    "# Separar las características y la variable objetivo\n",
    "X = result_df[features]\n",
    "y = result_df[target]\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=11101)\n",
    "\n",
    "# Crear un transformador para manejar las variables categoricas\n",
    "categorical_features = ['Id']\n",
    "numeric_features = ['Hour', 'Minutes', 'Intensity','Calories']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo de prediccion: Red Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.8407210940159974\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Crear el pipeline con preprocesamiento y el modelo MLPRegressor\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', MLPRegressor(random_state=42))])\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print(\"R2 score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicción con red de neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones en el conjunto de datos de prueba:\n",
      "[76.70341006 89.85719107 60.24091224 ... 76.03541286 57.41971837\n",
      " 61.83782743]\n"
     ]
    }
   ],
   "source": [
    "# Hacer predicciones en el conjunto de datos de prueba\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Imprimir las predicciones\n",
    "print(\"Predicciones en el conjunto de datos de prueba:\")\n",
    "print(predictions)\n",
    "\n",
    "\n",
    "# # Filtrar el conjunto de datos para obtener instancias asociadas a la Id específica\n",
    "# #future_data = result_df[result_df['Id'] == 2022484408]\n",
    "\n",
    "# # Verificar si hay datos asociados a la Id específica\n",
    "# #if future_data.empty:\n",
    "# #    print(\"No hay datos asociados a la Id 'mi_id'.\")\n",
    "# #else:\n",
    "#     # Preprocesar los datos usando el preprocesador definido anteriormente\n",
    "# #    X_future = future_data[features]\n",
    "#     print(X_future)\n",
    "#     #X_future_processed = preprocessor.transform(X_future)\n",
    "    \n",
    "#     # Hacer la predicción utilizando el modelo de red neuronal\n",
    "#     predictions = pipeline.predict(X_future)\n",
    "\n",
    "#     # Imprimir las predicciones\n",
    "#     print(\"Predicciones de HeartRate para la Id 'mi_id' en el futuro:\")\n",
    "#     print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b729f3",
   "metadata": {},
   "source": [
    "Modelo de prediccion : Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d3bebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el modelo de Random Forest\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=11101)\n",
    "\n",
    "# Crear el pipeline que combina el preprocesamiento y el modelo\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "score = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f'R2 Score on Test Set: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25fe11",
   "metadata": {},
   "source": [
    "Modelo de prediccion : KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7922051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Crear el modelo KNN Regressor\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=5) \n",
    "\n",
    "# Construir el pipeline con el transformador y el modelo\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', knn_regressor)\n",
    "])\n",
    "\n",
    "# Entrenar el modelo en los datos de entrenamiento\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b6bb93",
   "metadata": {},
   "source": [
    "Modelo de prediccion : Decision Tree Regressor (No tiene TransferLearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a084c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Crear el modelo DecisionTreeRegressor\n",
    "dt_regressor = DecisionTreeRegressor(random_state=11101)\n",
    "\n",
    "# Construir el pipeline con el transformador y el modelo\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', dt_regressor)\n",
    "])\n",
    "\n",
    "# Realizar validación cruzada\n",
    "scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "mse_scores = -scores  # Convertir a positivo, ya que cross_val_score devuelve la negación del MSE\n",
    "\n",
    "# Entrenar el modelo en el conjunto de entrenamiento\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calcular y mostrar la matriz de confusión\n",
    "mse_test = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error on Test Set: {mse_test}')\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(np.round(np.array([[mse_test]]), decimals=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987eda3",
   "metadata": {},
   "source": [
    "A continuacion implemntamos el KNN Regressor para que el modelo sea mas preciso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff39923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c4a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar las características y la variable objetivo\n",
    "X = result_df[['Id', 'Hour', 'Minutes', 'Weekday', 'Intensity', 'Calories']]\n",
    "y = result_df['HeartRate']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=11101)\n",
    "\n",
    "# Construir el pipeline con el transformador y el modelo KNN Regressor\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "pipeline_knn = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', knn_regressor)\n",
    "])\n",
    "\n",
    "# Entrenar el modelo KNN en los datos de entrenamiento\n",
    "pipeline_knn.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred_knn = pipeline_knn.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo KNN en el conjunto de prueba\n",
    "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
    "r2_knn = r2_score(y_test, y_pred_knn)\n",
    "\n",
    "print(f'Mean Squared Error (KNN): {mse_knn}')\n",
    "print(f'R-squared (KNN): {r2_knn}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
